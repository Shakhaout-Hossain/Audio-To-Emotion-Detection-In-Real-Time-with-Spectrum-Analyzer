<!-- <!DOCTYPE html>
<html>
<head>
    <title>Emotion Chart</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <div style="width: 800px; margin: 20px auto">
        <canvas id="emotionChart"></canvas>
    </div>

    <script>
        // Emotion data configuration
        const emotionData = {
            labels: ['Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fear', 'Disgust', 'Surprise'],
            datasets: [{
                label: 'Emotion Intensity',
                data: [3, 8, 9, 3, 6, 4, 2, 7], // Sample data (modify as needed)
                backgroundColor: [
                    '#9CA3AF', // Neutral
                    '#3B82F6', // Calm
                    '#FCD34D', // Happy
                    '#6B7280', // Sad
                    '#EF4444', // Angry
                    '#7C3AED', // Fear
                    '#10B981', // Disgust
                    '#F59E0B'  // Surprise
                ],
                borderWidth: 2
            }]
        };

        // Chart configuration
        const config = {
            type: 'bar', // Change to 'pie' or 'doughnut' for different formats
            data: emotionData,
            options: {
                responsive: true,
                plugins: {
                    legend: {
                        position: 'top',
                    },
                    title: {
                        display: true,
                        text: 'Emotion Distribution'
                    }
                },
                scales: {
                    y: {
                        beginAtZero: true,
                        title: {
                            display: true,
                            text: 'Intensity Level'
                        }
                    }
                }
            }
        };

        // Render the chart
        const emotionChart = new Chart(
            document.getElementById('emotionChart'),
            config
        );
    </script>
</body>
</html> -->



<!-- <!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Merge Audio Chunks and Play</title>
</head>
<body>
  <h2>Record and Merge Audio Chunks</h2>
  
  <button onclick="startRecording()">Start Recording</button>
  <button onclick="stopRecording()">Stop Recording</button>
  
  <h3>Player for Merged Audio:</h3>
  <audio id="audioPlayer" controls></audio>
  
  <script>
    let audioChunks = []; // Array to store recorded audio chunks
    let mediaRecorder;
    
    // Start recording
    function startRecording() {
      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
          mediaRecorder = new MediaRecorder(stream);
          
          mediaRecorder.ondataavailable = event => {
            audioChunks.push(event.data); // Store audio chunk
          };
          
          mediaRecorder.start();
        })
        .catch(error => {
          console.error('Error accessing microphone: ', error);
        });
    }
    
    // Stop recording and merge the audio chunks
    function stopRecording() {
      if (mediaRecorder) {
        mediaRecorder.stop();
      }

      // Merge audio chunks
      mergeAudioChunks(audioChunks).then(mergedAudioBlob => {
        // Create a URL for the merged audio blob
        const audioUrl = URL.createObjectURL(mergedAudioBlob);
        
        // Set the merged audio URL to the player
        const audioPlayer = document.getElementById('audioPlayer');
        audioPlayer.src = audioUrl;
      });
    }
    
    // Function to merge audio chunks and return the merged audio Blob
    async function mergeAudioChunks(audioChunks) {
      const audioContext = new (window.AudioContext || window.webkitAudioContext)();
      let combinedBuffer = null;

      for (let chunk of audioChunks) {
        let audioBuffer = await decodeAudioFile(audioContext, chunk);

        if (combinedBuffer === null) {
          combinedBuffer = audioBuffer;
        } else {
          let newBuffer = audioContext.createBuffer(
            audioBuffer.numberOfChannels,
            combinedBuffer.length + audioBuffer.length,
            audioContext.sampleRate
          );

          for (let channel = 0; channel < combinedBuffer.numberOfChannels; channel++) {
            newBuffer.getChannelData(channel).set(combinedBuffer.getChannelData(channel));
          }

          for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
            newBuffer.getChannelData(channel).set(audioBuffer.getChannelData(channel), combinedBuffer.length);
          }

          combinedBuffer = newBuffer;
        }
      }

      // Convert the combined buffer to a Blob (WAV format)
      return audioBufferToBlob(combinedBuffer);
    }

    // Decode the audio file into an AudioBuffer
    function decodeAudioFile(audioContext, audioFile) {
      return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.onload = (event) => {
          audioContext.decodeAudioData(event.target.result, (buffer) => {
            resolve(buffer);
          }, reject);
        };
        reader.readAsArrayBuffer(audioFile);
      });
    }

    // Convert AudioBuffer to Blob (WAV format)
    function audioBufferToBlob(audioBuffer) {
      const wavEncoder = new WaveFile();
      wavEncoder.fromScratch(audioBuffer.numberOfChannels, audioBuffer.length, audioBuffer.sampleRate);
      for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
        wavEncoder.channelData[channel] = audioBuffer.getChannelData(channel);
      }
      const buffer = wavEncoder.toBuffer();
      return new Blob([buffer], { type: 'audio/wav' });
    }
  </script>
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/wavefile/1.2.7/wavefile.min.js"></script>
</body>
</html> -->



<!-- <!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Merge Audio Chunks and Play</title>
</head>
<body>
  <h2>Record and Merge Audio Chunks</h2>
  
  <button onclick="startRecording()">Start Recording</button>
  <button onclick="stopRecording()">Stop Recording</button>
  
  <h3>Player for Merged Audio:</h3>
  <audio id="audioPlayer" controls></audio>
  
  <script>
    let audioChunks = []; // Array to store recorded audio chunks
    let mediaRecorder;
    
    // Start recording
    function startRecording() {
      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
          mediaRecorder = new MediaRecorder(stream);
          
          mediaRecorder.ondataavailable = event => {
            audioChunks.push(event.data); // Store audio chunk
          };
          
          mediaRecorder.start();
        })
        .catch(error => {
          console.error('Error accessing microphone: ', error);
        });
    }
    
    // Stop recording and merge the audio chunks
    function stopRecording() {
      if (mediaRecorder) {
        mediaRecorder.stop();
      }

      // Merge audio chunks
      mergeAudioChunks(audioChunks).then(mergedAudioBlob => {
        // Create a URL for the merged audio blob
        const audioUrl = URL.createObjectURL(mergedAudioBlob);
        
        // Set the merged audio URL to the player
        const audioPlayer = document.getElementById('audioPlayer');
        audioPlayer.src = audioUrl;
      });
    }
    
    // Function to merge audio chunks and return the merged audio Blob
    async function mergeAudioChunks(audioChunks) {
      const audioContext = new (window.AudioContext || window.webkitAudioContext)();
      let combinedBuffer = null;

      for (let chunk of audioChunks) {
        let audioBuffer = await decodeAudioFile(audioContext, chunk);

        if (combinedBuffer === null) {
          combinedBuffer = audioBuffer;
        } else {
          let newBuffer = audioContext.createBuffer(
            audioBuffer.numberOfChannels,
            combinedBuffer.length + audioBuffer.length,
            audioContext.sampleRate
          );

          for (let channel = 0; channel < combinedBuffer.numberOfChannels; channel++) {
            newBuffer.getChannelData(channel).set(combinedBuffer.getChannelData(channel));
          }

          for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
            newBuffer.getChannelData(channel).set(audioBuffer.getChannelData(channel), combinedBuffer.length);
          }

          combinedBuffer = newBuffer;
        }
      }

      // Convert the combined buffer to a Blob (WAV format)
      return audioBufferToBlob(combinedBuffer);
    }

    // Decode the audio file into an AudioBuffer
    function decodeAudioFile(audioContext, audioFile) {
      return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.onload = (event) => {
          audioContext.decodeAudioData(event.target.result, (buffer) => {
            resolve(buffer);
          }, reject);
        };
        reader.readAsArrayBuffer(audioFile);
      });
    }

    // Convert AudioBuffer to Blob (WAV format)
    function audioBufferToBlob(audioBuffer) {
      const wavEncoder = new WaveFile();
      wavEncoder.fromScratch(audioBuffer.numberOfChannels, audioBuffer.length, audioBuffer.sampleRate);
      for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
        wavEncoder.channelData[channel] = audioBuffer.getChannelData(channel);
      }
      const buffer = wavEncoder.toBuffer();
      return new Blob([buffer], { type: 'audio/wav' });
    }
  </script>
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/wavefile/1.2.7/wavefile.min.js"></script>
</body>
</html> -->









<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Visualizer with Auto-Scroll</title>
    <style>
        body { text-align: center; background: #000; color: #fff; font-family: Arial, sans-serif; }
        #visualization-container {
            width: 80%;
            margin: 20px auto;
        }
        canvas { 
            background: #000; 
            display: block; 
            margin: 10px auto;
        }
        #realTimeSpectrum { height: 150px; }
        #waveform { height: 100px; }
        #spectrum-container {
            display: flex;
            overflow-x: auto;
            padding: 10px;
            background: #111;
            scroll-behavior: smooth;
        }
        .segment {
            text-align: center;
            margin-right: 15px;
            cursor: pointer;
            transition: transform 0.2s;
            flex-shrink: 0;
        }
        .segment:hover { transform: scale(1.05); }
        .active { box-shadow: 0 0 10px yellow; }
        .timestamp {
            font-size: 12px;
            color: #aaa;
            margin-top: 5px;
        }
        audio { width: 40%; margin: 10px auto; }
    </style>
</head>
<body>
    <audio id="audio" controls>
        <source src="your-audio-file.mp3" type="audio/mpeg">
        Your browser does not support the audio element.
    </audio>

    <div id="visualization-container">
        <canvas id="realTimeSpectrum"></canvas>
        <canvas id="waveform"></canvas>
    </div>

    <div id="spectrum-container"></div>

    <script>
        const audio = document.getElementById("audio");
        const spectrumContainer = document.getElementById("spectrum-container");
        const waveformCanvas = document.getElementById("waveform");
        const realTimeSpectrum = document.getElementById("realTimeSpectrum");
        const spectrumCtx = realTimeSpectrum.getContext("2d");

        let audioCtx, analyser, waveformAnalyser, source, bufferLength, dataArray, waveformArray;
        const segmentInterval = 4; // 4-second segments
        let segments = [];
        let timeoutId;

        audio.addEventListener('play', () => {
            if (!audioCtx) {
                setupAudioContext();
            }
            scheduleNextSegment();
        });

        audio.addEventListener('pause', () => {
            if (timeoutId) clearTimeout(timeoutId);
        });

        audio.addEventListener('seeked', () => {
            if (!audio.paused) scheduleNextSegment();
        });

        function setupAudioContext() {
            audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            
            analyser = audioCtx.createAnalyser();
            analyser.fftSize = 512;
            bufferLength = analyser.frequencyBinCount;
            dataArray = new Uint8Array(bufferLength);

            waveformAnalyser = audioCtx.createAnalyser();
            waveformAnalyser.fftSize = 2048;
            waveformArray = new Uint8Array(waveformAnalyser.frequencyBinCount);

            source = audioCtx.createMediaElementSource(audio);
            source.connect(analyser);
            source.connect(waveformAnalyser);
            analyser.connect(audioCtx.destination);

            spectrumContainer.innerHTML = '';
            segments = [];

            requestAnimationFrame(drawWaveform);
            requestAnimationFrame(drawRealTimeSpectrum);
            requestAnimationFrame(updateActiveSegment);
        }

        function scheduleNextSegment() {
            if (timeoutId) clearTimeout(timeoutId);
            
            const currentTime = audio.currentTime;
            const nextSegmentEnd = Math.floor(currentTime / segmentInterval + 1) * segmentInterval;
            const timeUntilNext = (nextSegmentEnd - currentTime) * 1000;

            timeoutId = setTimeout(() => {
                if (!audio.paused && !audio.ended) {
                    const segmentStart = nextSegmentEnd - segmentInterval;
                    if (segmentStart >= 0) {
                        captureSegment(segmentStart);
                    }
                    scheduleNextSegment();
                }
            }, Math.max(0, timeUntilNext));
        }

        function captureSegment(segmentStart) {
            analyser.getByteFrequencyData(dataArray);
            if (!segments.some(s => s.dataset.start === segmentStart.toString())) {
                renderSegment(new Uint8Array(dataArray), segmentStart);
                spectrumContainer.scrollLeft = spectrumContainer.scrollWidth;
            }
        }

        function renderSegment(segmentData, segmentStart) {
            const segmentDiv = document.createElement("div");
            segmentDiv.className = "segment";
            
            const canvas = document.createElement("canvas");
            canvas.width = 120;
            canvas.height = 120;
            
            const timestampLabel = document.createElement("div");
            timestampLabel.className = "timestamp";
            timestampLabel.textContent = `${formatTime(segmentStart)} - ${formatTime(segmentStart + segmentInterval)}`;
            
            segmentDiv.appendChild(canvas);
            segmentDiv.appendChild(timestampLabel);
            segmentDiv.dataset.start = segmentStart;
            segmentDiv.addEventListener("click", () => {
                audio.currentTime = segmentStart;
            });

            spectrumContainer.appendChild(segmentDiv);
            segments.push(segmentDiv);
            drawSpectrum(canvas, segmentData);
        }

        function updateActiveSegment() {
            requestAnimationFrame(updateActiveSegment);
            const currentTime = audio.currentTime;

            segments.forEach(segment => {
                const segmentStart = parseFloat(segment.dataset.start);
                segment.classList.toggle("active", currentTime >= segmentStart && currentTime < segmentStart + segmentInterval);
            });
        }

        function formatTime(seconds) {
            const minutes = Math.floor(seconds / 60);
            const secs = Math.floor(seconds % 60);
            return `${minutes}:${secs < 10 ? "0" : ""}${secs}`;
        }
    </script>
</body>
</html>
